<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Centos下Spark的安装 | Gason</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="#Centos下Spark的安装与配置
##安装JDK
安装JDK大致分为下面4个步骤用户可以在Oracle JDK的官网下载相应版本的JDK，本例以JDK 1.6为例，官网地址为http://www.oracle.com/technetwork/java/javase/downloads/index.html

下载后，在解压出的JDK的目录下执行bin文件
./jdk-6u38-ea-bin-">
<meta property="og:type" content="article">
<meta property="og:title" content="Centos下Spark的安装">
<meta property="og:url" content="http://yoursite.com/2016/07/20/spark安装/index.html">
<meta property="og:site_name" content="Gason">
<meta property="og:description" content="#Centos下Spark的安装与配置
##安装JDK
安装JDK大致分为下面4个步骤用户可以在Oracle JDK的官网下载相应版本的JDK，本例以JDK 1.6为例，官网地址为http://www.oracle.com/technetwork/java/javase/downloads/index.html

下载后，在解压出的JDK的目录下执行bin文件
./jdk-6u38-ea-bin-">
<meta property="og:updated_time" content="2016-07-22T08:06:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Centos下Spark的安装">
<meta name="twitter:description" content="#Centos下Spark的安装与配置
##安装JDK
安装JDK大致分为下面4个步骤用户可以在Oracle JDK的官网下载相应版本的JDK，本例以JDK 1.6为例，官网地址为http://www.oracle.com/technetwork/java/javase/downloads/index.html

下载后，在解压出的JDK的目录下执行bin文件
./jdk-6u38-ea-bin-">
  
    <link rel="alternate" href="/atom.xml" title="Gason" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Gason</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">add</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-spark安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2016/07/20/spark安装/" class="article-date">
  <time datetime="2016-07-20T12:48:32.000Z" itemprop="datePublished">2016-07-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Spark/">Spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Centos下Spark的安装
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>#Centos下Spark的安装与配置</p>
<p>##安装JDK</p>
<p>安装JDK大致分为下面4个步骤<br>用户可以在Oracle JDK的官网下载相应版本的JDK，本例以JDK 1.6为例，官网地址为<a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="external">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<ul>
<li><p>下载后，在解压出的JDK的目录下执行bin文件</p>
<pre><code>./jdk-6u38-ea-bin-b04-linux-amd64-31_oct_2012.bin
</code></pre></li>
<li><p>配置环境变量，在/etc/profile增加以下代码</p>
<pre><code>JAVA_HOME=/home/chengxu/jdk1.6.0_38 
PATH=$JAVA_HOME/bin:$PATH 
CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/jre/lib/dt.jar:$JAVA_HOME/jre/lib/tools.jar 
export JAVA_HOME PATH CLASSPATH
</code></pre></li>
<li><p>使profile文件更新生效</p>
</li>
<li><pre><code>./etc/profile
</code></pre><p>##安装Scala<br>Scala官网提供各个版本的Scala，用户需要根据Spark官方规定的Scala版本进行下载和安装。Scala官网地址为<a href="http://www.scala-lang.org/.以Scala-2.10为例进行介绍" target="_blank" rel="external">http://www.scala-lang.org/.以Scala-2.10为例进行介绍</a>.</p>
</li>
<li><p>下载scala-2.10.4.tgz</p>
</li>
<li>在目录下解压：tar -xzvf scala-2.10.4.tgz</li>
<li>配置环境变量，在/etc/profile中添加下面的内容</li>
<li><pre><code>export SCALA_HOME=/home/chengxu/scala-2.10.4/scala-2.10.4 
export PATH=${SCALA_HOME}/bin:$PATH
</code></pre></li>
<li>使profile文件更新生效</li>
<li><pre><code>./etc/profile
</code></pre><p>##配置SSH免密码登录</p>
<blockquote>
<p> 在集群管理和配置中有很多工具可以使用。例如，可以采用pssh等Linux工具在集群中分发与复制文件，用户也可以自己书写Shell、Python的脚本分发包。Spark的Master节点向Worker节点发命令需要通过ssh进行发送，用户不希望Master每发送一次命令就输入一次密码，因此需要实现Master无密码登录到所有Worker。<br>Master作为客户端，要实现无密码公钥认证，连接到服务端Worker。需要在Master上生成一个密钥对，包括一个公钥和一个私钥，然后将公钥复制到Worker上。当Master通过ssh连接Woker时，Worker就会生成一个随机数并用Master的公钥对随机数进行加密，发送给Worker。Master收到加密数之后再用私钥进行解密，并将解密数回传给Worker，Worker确认解密数无误之后，允许Master进行连接。这就是一个公钥认证过程，其间不需要用户手工输入密码，主要过程是将Master节点公钥复制到Worker节点上。<br>下面介绍如何配置Master与Worker之间的SSH免密码登录。</p>
</blockquote>
</li>
<li><p>在Master节点上，执行以下命令</p>
</li>
<li><pre><code>ssh-keygen -t rsa
</code></pre></li>
<li>打印日志执行以下命令</li>
<li><pre><code>Generating public/private rsa key pair. 
Enter file in which to save the key (/root/.ssh/id_rsa): 
/*回车，设置默认路径*/ 
Enter passphrase (empty for no passphrase): 
/*回车，设置空密码*/ 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa. 
Your public key has been saved in /root/.ssh/id_rsa.pub.
如果是root用户，则在/root/.ssh/目录下生成一个私钥id_rsa和一个公钥id_rsa.pub。
 把Master上的id_rsa.pub文件追加到Worker的authorized_keys内，以172.20.14.144 （Worker）节点为例
</code></pre></li>
<li><p>复制Master的id_rsa.pub文件</p>
</li>
<li><pre><code>scp id_rsa.pub root@172.20.14.144:/home
</code></pre></li>
<li>可使用pssh对全部节点分发</li>
<li><pre><code>登录172.20.14.144 (Worker节点)，执行以下命令。
cat /home/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys 
/*可使用pssh对全部节点分发*/
</code></pre>其他的Worker执行同样的操作。</li>
</ul>
<blockquote>
<p>注意：配置完毕，如果Master仍然不能访问Worker，可以修改Worker的authorized_keys文件的权限，命令为chmod 600 authorized_keys。</p>
</blockquote>
<p>##安装Hadoop</p>
<p>下面讲解Hadoop的安装过程和步骤。</p>
<p>###1.下载hadoop-2.2.0</p>
<ul>
<li>选取一个Hadoop镜像网址，下载Hadoop（官网地址为<a href="http://hadoop.apache.org/）" target="_blank" rel="external">http://hadoop.apache.org/）</a></li>
<li><pre><code>$ wget http://www.trieuvan.com/apache/hadoop/common/ hadoop-2.2.0/hadoop-2.2.0.tar.gz
</code></pre></li>
<li>解压tar包</li>
<li><pre><code>$ sudo tar-vxzf hadoop-2.2.0.tar.gz -C /usr/local 
$ cd /usr/local 
$ sudo mv hadoop-2.2.0 hadoop 
$ sudo chown -R hduser:hadoop hadoop
</code></pre><p>###2. 配置Hadoop环境变量</p>
<ul>
<li>编辑profile文件</li>
<li><pre><code>vi /etc/profile
</code></pre></li>
<li><p>在profile文件中增加以下内容</p>
</li>
<li><pre><code>export JAVA_HOME=/usr/lib/jvm/jdk/ 
export HADOOP_INSTALL=/usr/local/hadoop 
export PATH=$PATH:$HADOOP_INSTALL/bin 
export PATH=$PATH:$HADOOP_INSTALL/sbin 
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL 
export HADOOP_COMMON_HOME=$HADOOP_INSTALL 
export HADOOP_HDFS_HOME=$HADOOP_INSTALL 
export YARN_HOME=$HADOOP_INSTALL
</code></pre>通过如上配置就可以让系统找到JDK和Hadoop的安装路径。</li>
</ul>
</li>
</ul>
<p>###3. 编辑配置文件</p>
<ul>
<li><p>进入Hadoop所在目录/usr/local/hadoop/etc/hadoop</p>
</li>
<li><p>配置hadoop-env.sh文件</p>
</li>
<li><pre><code>export JAVA_HOME=/usr/lib/jvm/jdk/
</code></pre></li>
<li>配置core-site.xml文件（主配置文件）</li>
<li><pre><code>&lt;configuration&gt;
/*这里的值指的是默认的HDFS路径*/ 
&lt;property&gt;
&lt;name&gt;fs.defaultFS&lt;/name&gt;
&lt;value&gt;hdfs://Master:9000&lt;/value&gt;
&lt;/property&gt;
/*缓冲区大小:io.file.buffer.size默认是4KB*/ 
&lt;property&gt;
&lt;name&gt;io.file.buffer.size&lt;/name&gt;
&lt;value&gt;131072&lt;/value&gt;
&lt;/property&gt;
/*临时文件夹路径*/ 
&lt;property&gt;
&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
&lt;value&gt;file:/home//tmp&lt;/value&gt;
&lt;description&gt;Abase for other  
temporary directories.       &lt;/description&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.hosts&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;hadoop.proxyuser.hduser.groups&lt;/name&gt;
&lt;value&gt;*&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></li>
<li>配置yarn-site.xml文件（资源调度文件）</li>
<li><pre><code>&lt;configuration&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
/*resourcemanager的地址*/ 
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
&lt;value&gt;Master:8032&lt;/value&gt;
&lt;/property&gt;
/*调度器的端口*/ 
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
&lt;value&gt; Master1:8030&lt;/value&gt;
&lt;/property&gt;
/*resource-tracker端口*/ 
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
&lt;value&gt; Master:8031&lt;/value&gt;
&lt;/property&gt;
/*resourcemanager管理器端口*/ 
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
&lt;value&gt; Master:8033&lt;/value&gt;
&lt;/property&gt;
/* ResourceManager 的 Web 端口，监控 job 的资源调度*/ 
&lt;property&gt;
&lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
&lt;value&gt; Master:8088&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre></li>
<li><p>配置mapred-site.xml文件，加入如下内容</p>
</li>
<li><pre><code>&lt;configuration&gt;
/*hadoop对map-reduce运行框架一共提供了3种实现，在mapred-site.xml中通过&quot;mapreduce.framework.name&quot;这个属性来设置为&quot;classic&quot;、&quot;yarn&quot;或者&quot;local&quot;*/ 
&lt;property&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;
&lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
/*MapReduce JobHistory Server地址*/ 
&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
&lt;value&gt;Master:10020&lt;/value&gt;
&lt;/property&gt;
/*MapReduce JobHistory Server Web UI地址*/ 

&lt;property&gt;
&lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;
&lt;value&gt;Master:19888&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>###4. 创建namenode和datanode目录，并配置其相应路径</p>
</li>
<li>创建namenode和datanode目录，执行以下命令</li>
<li><pre><code>$ mkdir /hdfs/namenode 
$ mkdir /hdfs/datanode
+ 执行命令后，再次回到目录 /usr/local/hadoop/etc/hadoop，配置hdfs-site.xml 文件，在文件中添加如下内容。
&lt;configuration&gt;
/*配置主节点名和端口号*/ 
&lt;property&gt;
&lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
&lt;value&gt;Master:9001&lt;/value&gt;
&lt;/property&gt;
/*配置从节点名和端口号*/ 
&lt;property&gt;
&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
&lt;value&gt;file:/hdfs/namenode&lt;/value&gt;
&lt;/property&gt;
/*配置datanode的数据存储目录*/ 
&lt;property&gt;
&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
&lt;value&gt;file:/hdfs/datanode&lt;/value&gt;
&lt;/property&gt;
/*配置副本数*/ 
&lt;property&gt;
&lt;name&gt;dfs.replication&lt;/name&gt;
&lt;value&gt;3&lt;/value&gt;
&lt;/property&gt;
/*将dfs.webhdfs.enabled属性设置为true，否则就不能使用webhdfs的LISTSTATUS、LISTFILESTATUS等需要列出文件、文件夹状态的命令，因为这些信息都是由namenode保存的*/ 
&lt;property&gt;
&lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;
&lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre><p>###5.配置Master和Slave文件</p>
<pre><code>1）Master文件负责配置主节点的主机名。例如，主节点名为Master，则需要在Master文件添加以下内容。
        Master /*Master为主节点主机名*/
2）配置Slaves文件添加从节点主机名，这样主节点就可以通过配置文件找到从节点，和从节点进行通信。例如，以Slave1～Slave5为从节点的主机名，就需要在Slaves文件中添加如下信息。
    /Slave*为从节点主机名*/ 
    Slave1 
    Slave2 
    Slave3 
    Slave4 
    Slave5
</code></pre><p>###6. 将Hadoop的所有文件通过pssh分发到各个节点</p>
<pre><code>执行如下命令: ./pssh -h hosts.txt -r /hadoop  /
</code></pre><p>###7. 格式化Namenode（在Hadoop根目录下）</p>
<pre><code>./bin/hadoop namenode -format
</code></pre><p>###8. 启动Hadoop</p>
<pre><code>./sbin/start-all.sh
</code></pre><p>###9. 查看是否配置和启动成功</p>
<pre><code>如果在x86机器上运行，则通过jps命令，查看相应的JVM进
2584 DataNode 
2971 ResourceManager 
3462 Jps 
3179 NodeManager 
2369 NameNode 
2841 SecondaryNameNode
注意，由于在IBM JVM中没有jps命令，所以需要用户按照下面命令逐个查看。
ps-aux|grep *DataNode*     /*查看DataNode进程*/
</code></pre></li>
</ul>
<p>##安装Spark</p>
<p>进入官网下载对应Hadoop版本的Spark程序包（见图2-1），官网地址为<a href="http://spark.apache.org/downloads.html。" target="_blank" rel="external">http://spark.apache.org/downloads.html。</a></p>
<p>以Spark1.0版本为例，介绍Spark的安装。</p>
<ul>
<li><p>下载spark-1.0.0-bin-hadoop2.tgz。</p>
</li>
<li><p>解压tar -xzvf spark-1.0.0-bin-hadoop2.tgz。</p>
</li>
<li><p>配置conf/spark-env.sh文件</p>
<ol>
<li><p>用户可以配置基本的参数，其他更复杂的参数请见官网的配置（Configuration）页面，Spark配置（Configuration）地址为：<a href="http://spark.apache.org/docs/latest/configuration.html。" target="_blank" rel="external">http://spark.apache.org/docs/latest/configuration.html。</a></p>
</li>
<li><p>编辑conf/spark-env.sh文件，加入下面的配置参数</p>
<pre><code>export SCALA_HOME=/path/to/scala-2.10.4 
export SPARK_WORKER_MEMORY=7g
export SPARK_MASTER_IP=172.16.0.140 
export MASTER=spark://172.16.0.140:7077
</code></pre><p> 参数SPARK_WORKER_MEMORY决定在每一个Worker节点上可用的最大内存，增加这个数值可以在内存中缓存更多数据，但是一定要给Slave的操作系统和其他服务预留足够的内存。</p>
<p> 需要配置SPARK_MASTER_IP和MASTER，否则会造成Slave无法注册主机错误。</p>
</li>
</ol>
</li>
<li><p>配置slaves文件。</p>
<pre><code>编辑conf/slaves文件，以5个Worker节点为例，将节点的主机名加入slaves文件中。

Slave1 
Slave2 
Slave3 
Slave4 
Slave5
</code></pre><p>##启动集群</p>
</li>
<li><p>Spark启动与关闭</p>
</li>
<li><pre><code>在Spark根目录启动Spark: ./sbin/start-all.sh
</code></pre></li>
<li><p>关闭Spark</p>
<pre><code>./sbin/stop-all.sh
</code></pre></li>
<li>Hadoop的启动与关闭</li>
<li><pre><code>1）在Hadoop根目录启动Hadoop
    ./sbin/start-all.sh
2）关闭Hadoop
    ./sbin/stop-all.sh
</code></pre></li>
<li><p>检测是否安装成</p>
<p>  1）正常状态下的Master节点如下。</p>
<pre><code>-bash-4.1# jps 
23526 Jps 
2127 Master 
7396 NameNode     
7594 SecondaryNameNode 
7681 ResourceManager
</code></pre><p>  2）利用ssh登录Worker节点</p>
<pre><code>-bash-4.1# ssh slave2 
-bash-4.1# jps 
1405 Worker 
1053 DataNode 
22455 Jps 
31935 NodeManager
</code></pre></li>
</ul>
<p>至此，在Linux集群上安装与配置Spark集群的步骤告一段落。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2016/07/20/spark安装/" data-id="ciqxgd5l700029tj6a8d2imdg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Spark/" style="font-size: 10px;">Spark</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/07/20/spark安装/">Centos下Spark的安装</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2016 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>